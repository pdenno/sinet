* Org Mode Tasks
** DONE Start using org for programming notes.
   CLOSED: [2017-10-14 Sat 18:06]
** DONE Learn how to 'open' and edit a link. Right now the org manual link is just the URL. 
   CLOSED: [2017-10-15 Sun 12:25]
** TODO Find out what good the source code marking is. I see that you can edit it with C-c ' but so what?

* Notes on org mode
** Useful Links
*** The tutorial 
 [[http://orgmode.org/worg/org-tutorials/orgtutorial_dto.html][The tutorial]]
 http://orgmode.org/manual/index.html
    
** Key bindings 
*** Opening and closing
**** Shift-tab open/close whole document (circular)
**** Shift-cntl open/close current
*** Linking
**** C-c l (store link)
**** C-c C-l bring the link here
**** C-c C-l also to edit a link (when point is on the link)
**** Links look like this: [[link][description] ...] 
  Here I inserted elipsis to stop that from turning into a link. An actual 
  link to a web page would appear like this: [[http://orgmode.org/worg/org-tutorials/orgtutorial_dto.html][the tutorial]]. 
**** C-c C-o (open) open a link without the mouse.   
    This is especially sweet for local links. It splits showing two buffers. 

*** TODO Operations
**** M-Shift-Ret open a TODO.

**** C-c C-t (terminate?) marks a TODO as done.
*** Miscellaneous
**** Timestamps C-c . (C-c ! is similar but 'without creating an agenda entry' whatever that means.)

**** Source Code C-c ' (open and close) #+BEGIN_SRC clojure ... #+END_SRC


* Sinet TODOs
** DONE Try a more mild blocking situation.
   CLOSED: [2017-10-23 Mon 19:05]
** DONE Fix bug in pnr/simple-reach. See bug <2017-10-15 Sun>
   CLOSED: [2017-10-15 Sun 18:13]
** TODO Consider the possibility/advantages of an 'abbreviated' reachability graph. 
** TODO Decide whether setting all transitions to timed makes sense for simple-reach. See discussion <2017-10-15 Sun>
** TODO Investigate possible bugs with MJPdes 'coordination' and create more 'deterministic' test cases.
** TODO Conceive of a strategy for using exceptional fitness
 [I think I missed the whole point in the stuff below, but just to be sure: There are messages that
  are not associated with transitions. They are associated with STATES. I may have a place in the network
  that is essentially "M1-BLOCKED" but it won't be labelled as such! A nice thing about this is that 
  I DON'T WANT to get a M1-BLOCKED message every (vanishing) time that there is a token in that place.
  I only want these when the buffer is filled (a state where the buffer place looms large). That's what 
  will happen, and we don't have to worry about labeling the "m1-blocked" place!]

 In some cases like blocking/starving it seem that having a place represent the state is appropriate. 
 But is that generally the case? This ToDo is closely related to the next about vanishing transitions,
 but I don't think it is the same issue. I think I need to see the result of the "mild blocking" 
 situation before I can sort this out. Remember that the goal is to get rates out of these things. 

 (More on this): I think there might be quirk to what I'm trying to do: I'd like to allow the PN to handle 
 exceptional messages just as some do for block/starve, but block/starve are places, not transitions, and 
 they won't have meaningful names. I think I'll just wait to explore this after getting the exceptional 
 fitness hooked in. 

** TODO Deal with vanishing transitions. 
 I need to generate blocking/starving messages ONLY when the net sojourns at blocking/starving. 
 So simulation has to be better and I need to distinguish these immediate transitions some how. 

** TODO Find a way to find the pathname "to a namespace" so I can run tests on dependent packages. 
** TODO Create a project for simple neural nets. Sinet log date <2017-10-23 Mon> has some code for it. 

** DONE Fix MJPdes as described in log <2017-10-17 Tue>
   CLOSED: [2017-10-22 Sun 14:25]
* Discussion
** Rationale for PNN
It is clear that I need a means to explain messages that do not correspond to transitions. 
I need to discover the pattern associated with these other messages, if such a pattern exists. 
The PNN is just the way to do this. Do we associate a semantics to these messages? We can assume
that in general that may not be possible, but there is a "PN semantics" in the case
of blocking and starvation that it would be useful to know. (Likewise for competition for
resources and deadlocking). In the blocking/starvation cases, there is still the matter
of determining where the problem buffer is. Once starving happens it continues upstream, 
and blocking continues downstream. (You could check for that in the causal model.)
Likewise could look at non-delivery of a part from a feeder line. 

*** Think about the role of PNN in causal modeling
The nice thing here is that using the parametric infinitessimal, I'll have real-numbered 
quantities of tokens in places. The PNN essentially shows the state changes while I move around the parameters.

*** Think about what is being inferred. 

*** What role do the "starving/blocking" places have? Can they be added?

*** Define the steps 
   - recognition of a pattern
   - hypothesis generation
   - hypothesis testing (causal? comparison?)
** IDEA: Maybe award fitness to PNs that represent it. 
***       Award a little less to those who can at least cope with it with a NN. 

* Sinet Log *
** <2017-10-14 Sat>
*** I started with this log file: [[file:~/Documents/git/sinet/data/SCADA-logs/m2-j1-n3-block-out.clj::%5B][m2-j1-n3-block-out.clj]] which blocks like crazy. 
*** I switched from returning 'the first' good starting marking to all markings.
*** I then compared the results from all three:

I am indexing the nets by the starting state. Each will associate a different marking with the
exceptional msg (defined as :fires-on). They all map [3 0 1 1 0] to the highest value of the NN.
Yet only the second one was trained for that outcome! The problem is that higher values are
going to drive the NN higher. (For example [10 0 1 1 0] give values even closer to 1 in each case.)
I can at least conclude that the one with the correct starting marking has the most prominent max
value (0.398 versus 0.213) is almost double, whereas the others are only  (0.57 versus 0.53) and
(0.71 vs 0.67) In all three cases, second best is [3 1 0 1 0].

**** (test-markings (get nnns [2 0 1 1 0]))   :fires-on {[2 0 1 1 0] 491}},
{[0 1 0 1 0] (0.0019047821808839684),
 [3 1 0 1 0] (0.5329793128418048),
 [2 0 1 1 0] (0.4557148825447303),
 [1 1 0 1 0] (0.028426845459858803),
 [3 0 1 0 1] (0.2520620080684257),
 [1 0 1 1 0] (0.0824105061449745),
 [0 1 0 0 1] (4.4899449786599065E-4),
 [2 1 0 1 0] (0.3112057574008602),
 [3 0 1 1 0] (0.5690740829317157),   ----- (next best is 0.53, which is :fires-on)
 [1 0 1 0 1] (0.007984861441426688),
 [3 1 0 0 1] (0.33543893173498296),
 [0 0 1 1 0] (0.0017534551604061264),
 [0 0 1 0 1] (0.0010127514448150464),
 [2 0 1 0 1] (0.04765593309469408)}

**** (test-markings (get nnns [3 0 1 1 0]))   :fires-on {[3 0 1 1 0] 491}},
{[0 1 0 1 0] (0.0014487996373689941),
 [3 1 0 1 0] (0.21293141920670883),
 [2 0 1 1 0] (0.15438051750729484),
 [1 1 0 1 0] (0.005105555396279919),
 [3 0 1 0 1] (0.05425027679413153),
 [1 0 1 1 0] (0.017136382643884614),
 [0 1 0 0 1] (4.6382801763844935E-4),
 [2 1 0 1 0] (0.03850346112492955),
 [3 0 1 1 0] (0.39880145491033164), ----- This IS fires-on. (next best is 0.213) 
 [1 0 1 0 1] (0.006230800133017719),
 [3 1 0 0 1] (0.03105553855253639),
 [0 0 1 1 0] (0.002015237400342786),
 [0 0 1 0 1] (0.001066242446042215),
 [2 0 1 0 1] (0.022575588402398115)}

**** (test-markings (get nnns [1 0 1 1 0]))   :fires-on {[1 0 1 1 0] 388}}}
{[0 1 0 1 0] (0.014569799679218615),
 [3 1 0 1 0] (0.6721408448530173),
 [2 0 1 1 0] (0.6688434016370715),
 [1 1 0 1 0] (0.21941460772717947),
 [3 0 1 0 1] (0.6601096030824811),
 [1 0 1 1 0] (0.46608997395936497),
 [0 1 0 0 1] (0.001252201438882003),
 [2 1 0 1 0] (0.5432656472113356),
 [3 0 1 1 0] (0.7113587186344846),  --- (next best is 0.672, which is [3 1 0 1 0]
 [1 0 1 0 1] (0.033295385315624364),
 [3 1 0 0 1] (0.5286157002824883),
 [0 0 1 1 0] (0.04039619976094015),
 [0 0 1 0 1] (0.00328347348876465),
 [2 0 1 0 1] (0.41021721559535307)}




**** Advantage: 
     The correct starting state has fewer false positives. It will do best. 

n**** Problem: 
     The algorithm is just going to focus on the buffer being high/low. This is fine for 
     blocking/starving events but not good for state-but-not-transition situations.

** <2017-10-15 Sun>
*** I ended up with data that blocks 30 times and starves 14 times: 
This was after about 30 experiments, fine tuning the parameters. The problem is that it
is very easy to get very short blocking/starving periods. 
Input:   file:~/Documents/git/sinet/data/SCADA-logs/m2-j1-n3-block-mild.clj]]
Output:  [[file:~/Documents/git/sinet/data/SCADA-logs/m2-j1-n3-block-mild-b30-s14.clj:::status%20nil,][file:~/Documents/git/sinet/data/SCADA-logs/m2-j1-n3-block-mild-b30-s14.clj]]
Pretty:  [[file:~/Documents/git/sinet/data/SCADA-logs/m2-j1-n3-block-mild-out.clj::%5B][file:~/Documents/git/sinet/data/SCADA-logs/m2-j1-n3-block-mild-out.clj]]
*** This was achieved with these parameters:
#+BEGIN_SRC clojure
  (map->Model
   {:line 
    {:m1 (map->ExpoMachine {:lambda 0.6 :mu 3.5 :W 1.0}) 
     :b1 (map->Buffer {:N 3})
     :m2 (map->ExpoMachine {:lambda 0.001 :mu 0.99 :W 1.0})}
    :number-of-simulations 1
    :report {:log? true :max-lines 3000}
    :topology [:m1 :b1 :m2]
    :entry-point :m1
    :params {:warm-up-time 2000 :run-to-time 10000}
    :jobmix {:jobType1 (map->JobType {:portion 1.0 :w {:m1 1.0, :m2 1.17}})}})
#+END_SRC
*** The m2-j1-n3 PN 
#+BEGIN_SRC clojure
{:places
 [{:name :buffer, :pid 0, :initial-tokens 0}
  {:name :m1-blocked, :pid 1, :initial-tokens 0}
  {:name :m1-busy, :pid 2, :initial-tokens 1}
  {:name :m2-busy, :pid 3, :initial-tokens 1}
  {:name :m2-starved, :pid 4, :initial-tokens 0}],
 :transitions
 [{:name :m1-complete-job, :tid 6, :type :exponential, :rate 0.9}
  {:name :m1-start-job, :tid 7, :type :immediate, :rate 1.0}
  {:name :m2-complete-job, :tid 8, :type :exponential, :rate 1.0}
  {:name :m2-start-job, :tid 9, :type :immediate, :rate 1.0}],
 :arcs
 [{:aid 10, :source :buffer, :target :m1-start-job, :name :aa-10, :type :inhibitor, :multiplicity 3, :bind {:jtype :blue}, :priority 1}
  {:aid 11, :source :buffer, :target :m2-start-job, :name :aa-11, :type :normal, :multiplicity 1, :bind {:jtype :blue}, :priority 1}
  {:aid 12, :source :m1-blocked, :target :m1-start-job, :name :aa-12, :type :normal, :multiplicity 1, :bind {:jtype :blue}, :priority 1}
  {:aid 13, :source :m1-busy, :target :m1-complete-job, :name :aa-13, :type :normal, :multiplicity 1, :bind {:jtype :blue}, :priority 1}
  {:aid 14, :source :m1-complete-job, :target :m1-blocked, :name :aa-14, :type :normal, :multiplicity 1, :bind {:jtype :blue}, :priority 1}
  {:aid 15, :source :m1-start-job, :target :buffer, :name :aa-15, :type :normal, :multiplicity 1, :bind {:jtype :blue}, :priority 2}
  {:aid 16, :source :m1-start-job, :target :m1-busy, :name :aa-16, :type :normal, :multiplicity 1, :bind {:jtype :blue}, :priority 1}
  {:aid 17, :source :m2-busy, :target :m2-complete-job, :name :aa-17, :type :normal, :multiplicity 1, :bind {:jtype :blue}, :priority 1}
  {:aid 18, :source :m2-complete-job, :target :m2-starved, :name :aa-18, :type :normal, :multiplicity 1, :bind {:jtype :blue}, :priority 1}
  {:aid 19, :source :m2-start-job, :target :m2-busy, :name :aa-19, :type :normal, :multiplicity 1, :bind {:jtype :blue}, :priority 1}
  {:aid 20, :source :m2-starved, :target :m2-start-job, :name :aa-20, :type :normal, :multiplicity 1, :bind {:jtype :blue}, :priority 1}]}
#+END_SRC 
 
*** Does it make sense to have an entry point with no :fire-ons?  (Yes, but...)
 I get two entry point markings, only one has anything in :fires-on. 
 I search for 50 steps supposedly, but it goes 225 lines, ending on the only exceptional msg, 
 which is {:act :m2-starved, :indx 225, :Mp [1 0 1 0 1]}. None of that should matter, we
 aren't looking for exceptional one yet. 
**** Is the problem that it is using the short data?
**** Yeah, ok this is wrong! : 
     (def foo (interpret-scada reach1 (-> (app-info) :problem :scada-log) lili))
     (count foo) ==> 225
     So I have been using the wrong data all the time, or it is stopping early?
     Good! It is stopping after 225 (the other goes 3000). I'm guessing that it gets 
     stuck in a situation it cannot interpret. Verify...
   
     Here is the new "failed-on" information:
     {:failed-prior   {:M [1 0 1 1 0], :fire :m2-complete-job, :Mp [1 0 1 0 1], :rate 1.0, :indx 224},
      :failed-on-link {:act :m2-starved, :indx 225, :Mp [1 0 1 0 1]},
      :failed-on-msg  {:act :m1-complete-job, :bf :b1, :j 1745, :n 0, :clk 2067.08452126566, :line 226, :mjpact :bj, :m :m1}}

     According to the reachability graph, the only thing that can occur after [1 0 1 0 1] is :m2-start-job
     {:M [1 0 1 0 1], :fire :m2-start-job, :Mp [0 0 1 1 0], :rate 1.0}
     That makes sense because ...
      [:buffer     1
       :m1-blocked 0
       :m1-busy    1
       :m2-busy    0
       :m2-starved 1]
      ... No, it doesn't make sense. [1 1 0 0 1] should also be possible. This is a vanishing transition to :m1-blocked.
***** BUG 
      The reachability graph must be wrong, but before I fix it, I won't build nets where :failed-on-msg is on last. DONE. 
      Something is seriously wrong. I created pnml for the N=3 PN but I don't get the same reachability as I did earlier
      (def reach1 ...) This one is much smaller. 

      Even the N=1 net is wrong. It should have 12 links, not 10:
      [[file:~/TwoDrive/OneDrive/Repo/mindmaps/images/m2-n1-no-immediate-reach.jpg]]

      First of all, these PNs have immediate transitions. 
          
      This is bad, there should be two here!    
    
      (next-links pnpn [0 1 0 1 0])
      [{:M [0 1 0 1 0], :fire :m1-start-job, :Mp [1 0 1 1 0], :rate 1.0}]
      The problem was that I was not setting all transitions to timed. THIS MAY NEED MORE THOUGHT (to TODO)
*** So now I have larger rgraphs (e.g. 28 vs 18 for N=3) do things still work?
**** DONE write code to generate simple-reach
     CLOSED: [2017-10-15 Sun 20:07]
**** write fitness assessment. 
*** Junk I'll probably never use
#_(defn prev-ordinary
  "Return an ordinary message, at index n or earlier."
  [data n]
  (loop [indx n]
    (cond (ordinary? (nth data indx)) (nth data indx), 
          (== indx 0) nil
          :otherwise (recur (dec indx)))))

(defn big-train
  ([net] (big-train net :m1-blocked 1))
  ([net msg-type cnt]
   (reduce (fn [n _] (train-msg n msg-type))
           net
           (range cnt))))

;;; POD This is for :m2-starved. 
(defn more-exceptional-training
  [net cnt]
  (reduce (fn [n _]
            (nn/train-step net [0.0 0.0 1.0 0.0 1.0] [1.0]))
          net
          (range cnt)))


*** DONE Write code to defobulate/zipmap (and pick best???)
    CLOSED: [2017-10-17 Tue 14:17]
** <2017-10-16 Mon>
 So far there is only one NN per message. 

I cleaned things up so that I get one NN per message. 

I studied Probabilistic Neural Nets briefly. They require one neuron for each training instance
and therefore for my application I think would be equivalent of a lookup table encompasing the whole
training set. I think they make more sense when there are points "between" the elements in the
training set. (Do I have these???) Needs more study, but maybe not so promising.

Let's look at how these things do against the 

** <2017-10-17 Tue>

*** Cortex
Cortex layer types (what I'm seeing [[https://github.com/thinktopic/cortex/commit/4be1c559675b9612249abbb94963d989d70817fe][here]]): convolutional, max-pooling, dropout, relu, linear, softmax.
But *this* matters: [[https://machinelearningmastery.com/confusion-matrix-machine-learning/][Confusion matrix]]: Describes what one is and how to calculate one for the 2-class
classification problem!

[[https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/][Nice discussion of use of layers in CNN]].

**** Dropout 
     Seems to be primarily about overfitting and regularization - introducing additional information to solve ill-posed problems
     Regularization is NOT what I want. 

     "The idea of dropout is simplistic in nature. This layer “drops out” a random set of activations in that 
      layer by setting them to zero. Simple as that. Now, what are the benefits of such a simple and seemingly 
      unnecessary and counterintuitive process? Well, in a way, it forces the network to be redundant. 
      By that I mean the network should be able to provide the right classification or output for a specific 
      example even if some of the activations are dropped out"

**** Max-pooling
     Used in convolutional NNs. Use biggest value on a window. [[https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/pooling_layer.html][example here]]. The goal is to reduce spatial dimensions (but not depth)
     on a convolutional NN. Not what I want.

**** ReLu [[https://stackoverflow.com/questions/27319931/relu-and-dropout-in-cnn][here]]. (Rectified Linear Unit, Not a layer, the activation function of a single neuron.)
     The rectifier function is an activation function f(x) = Max(0, x) which can be used by neurons just like 
     any other activation function, a node using the rectifier activation function is called a ReLu node. 
     The main reason that it is used is because of how efficiently it can be computed compared to more conventional 
     activation functions like the sigmoid and hyperbolic tangent, without making a significant difference to 
     generalisation accuracy. The rectifier activation function is used instead of a linear activation function to 
     add non linearity to the network, otherwise the network would only ever be able to compute a linear function.

     This part sounds useful "to add non-linearity to the network, otherwise the network would only ever be able to
     compute a linear function." That is the problem I'm experiencing now. 

**** Convolutional 
     They aren't fully connected...

**** Softmax (Not a layer, the activation function of a single neuron.)
     In mathematics, the softmax function, or normalized exponential function is a generalization of the 
     logistic function that "squashes" a K-dimensional vector z of arbitrary real values to a K-dimensional vector 
      {\displaystyle \sigma (\mathbf {z} )} \sigma (\mathbf {z} ) of real values in the range [0, 1] that add up to 1.
     (wikipedia)
     [[https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions][Softmax and ReLU]].

**** Probabilistic Neural Nets
   [[https://web.archive.org/web/20101218121158/http://herselfsai.com/2007/03/probabilistic-neural-networks.html][Start here]].


*** Confusion matrix
"A confusion matrix is a technique for summarizing the performance of a classification algorithm."
Thus if I have one of these, I can use it directly to determine the fitness of the combination of
PN + NNs as an identification of the system. 

Easy enough. Should also look at precision, recall, specificity and sensitivity. 
These are all defined on this [[https://en.wikipedia.org/wiki/Confusion_matrix][wikipedia page]]. I also have a paper by D. M. W. Powers in the ML
section of Mendeley.

*** fitness.clj
I think before I go much further, I have to fix the problem any problems deciding the class. 
I had hoped that it was just a matter of choosing marking > 0.5. Is this the case? 

HELP! No marking hits on any exceptional message. I don't think I have looked at results
since the "dense" exceptional messages of Saturday. 

*** What does the marking used as input to training mean?
    I was hoping that it was the marking just before the message is issued. 
    Messages are issued on complete-job / start-job but MJPdes doesn't 
    order these where they all happen simultaneously. Maybe it should???

[:buffer :m1-blocked :m1-busy :m2-busy :m2-starved]
 {:msg-type :m1-blocked,   [3 0 1 1 0] 30} ... I assume next msg is m1-complete-job
 {:msg-type :m1-unblocked, [2 1 0 1 0] 30} ... 
 {:msg-type :m2-starved,   [0 0 1 0 1] 14} 
 {:msg-type :m2-unstarved, [0 0 1 1 0] 14} 
 
Let's generate the interpretation [[file:~/Documents/git/sinet/data/SCADA-logs/m2-j2-n3-block-mild-interpreted.clj::{:act%20:m1-blocked,%20:prev-act%20:m1-start-job,%20:indx%20710,%20:Mp%20%5B3%200%201%201%200%5D,%20:clk%202206.0879216608246}][AND SAVE IT AS A FILE]]. 

Start link is the same for all 4 message types, yet I interpret the log four times. Nice. 
    
**** Typical output with "data/SCADA-logs/m2-j1-n3-block-mild-out.clj"       
gov.nist.sinet.fitness> (ppprint (zipmap markings1
                                         (map #(first (nn/eval-net (:m1-blocked nnns) %)) markings1)))
{[0 1 0 1 0] 0.024485036900973763,
 [3 1 0 1 0] 0.008046179155291305,
 [2 0 1 1 0] 0.004074497484690655,
 [1 1 0 1 0] 0.013784127155537253,
 [3 0 1 0 1] 0.004062515347353122,
 [1 0 1 1 0] 0.005371589317720032,
 [0 1 0 0 1] 0.0521152547076084, <-- wrong, and next best is 0.027
 [2 1 0 1 0] 0.009709532322778542,
 [3 0 1 1 0] 0.003524192574949857,
 [1 0 1 0 1] 0.009338700112664192,
 [3 1 0 0 1] 0.010161648095898495,
 [0 0 1 1 0] 0.008800298059058962,
 [1 1 0 0 1] 0.026920570777390792,
 [0 0 1 0 1] 0.01845261113215176,
 [2 1 0 0 1] 0.014811122272394004,
 [2 0 1 0 1] 0.0055072388527700866}
nil
gov.nist.sinet.fitness> (ppprint (zipmap markings1
                                         (map #(first (nn/eval-net (:m1-unblocked nnns) %)) markings1)))
{[0 1 0 1 0] 0.025163102369711258,
 [3 1 0 1 0] 0.016114179019312516,
 [2 0 1 1 0] 0.007977926858371297,
 [1 1 0 1 0] 0.0201854362883496,
 [3 0 1 0 1] 0.014301613606198235,
 [1 0 1 1 0] 0.008978827733614799,
 [0 1 0 0 1] 0.06621052090624126,  <--- wrong, next is 0.036
 [2 1 0 1 0] 0.01751290729163327,
 [3 0 1 1 0] 0.007622303706912641,
 [1 0 1 0 1] 0.018487183654880144,
 [3 1 0 0 1] 0.029438724652992698,
 [0 0 1 1 0] 0.01104428285399589,
 [1 1 0 0 1] 0.04735457640118155,
 [0 0 1 0 1] 0.02363169658896575,
 [2 1 0 0 1] 0.036172647847275446,
 [2 0 1 0 1] 0.01569931368097529}
nil
gov.nist.sinet.fitness> (ppprint (zipmap markings1
                                         (map #(first (nn/eval-net (:m2-starved nnns) %)) markings1)))

[:buffer :m1-blocked :m1-busy :m2-busy :m2-starved]
{[0 1 0 1 0] 0.017027576075899276,
 [3 1 0 1 0] 0.004886266737864184,
 [2 0 1 1 0] 0.006935985604851713,
 [1 1 0 1 0] 0.008132810057098754,
 [3 0 1 0 1] 0.008690333194503956,
 [1 0 1 1 0] 0.011443272037409967,
 [0 1 0 0 1] 0.06431606145668564,
 [2 1 0 1 0] 0.005697633024429593,
 [3 0 1 1 0] 0.005600030048084727,
 [1 0 1 0 1] 0.03823100286076116,
 [3 1 0 0 1] 0.00657718778677882,
 [0 0 1 1 0] 0.029978431187203013,
 [1 1 0 0 1] 0.019941922787321802,
 [0 0 1 0 1] 0.11582262659576607, <-- yes. next is 0.064
 [2 1 0 0 1] 0.00937067227357531,
 [2 0 1 0 1] 0.014909753268535426}
nil
gov.nist.sinet.fitness> (ppprint (zipmap markings1
                                         (map #(first (nn/eval-net (:m2-unstarved nnns) %)) markings1)))
[:buffer :m1-blocked :m1-busy :m2-busy :m2-starved]
{[0 1 0 1 0] 0.026026209106502097,
 [3 1 0 1 0] 0.0034988958592581513,
 [2 0 1 1 0] 0.00601129014254703,
 [1 1 0 1 0] 0.010186463188839843,
 [3 0 1 0 1] 0.0033239310083816543,
 [1 0 1 1 0] 0.011904922484223533,
 [0 1 0 0 1] 0.016127654355549154,
 [2 1 0 1 0] 0.00529834749317781,
 [3 0 1 1 0] 0.00385956712448291,
 [1 0 1 0 1] 0.008798843571705668,
 [3 1 0 0 1] 0.0029969968812421986,
 [0 0 1 1 0] 0.03188710693160681, <--- I guess...what does the marking mean???
 [1 1 0 0 1] 0.007127766343136932,
 [0 0 1 0 1] 0.020988809038561193,
 [2 1 0 0 1] 0.004186967134676599,
 [2 0 1 0 1] 0.00488250449212974}
nil
gov.nist.sinet.fitness> 


Let's go back to the dense messages and see if we still get good results.



Some bad news (Sort of. Hey, we need an interesting paper!): With more exceptional instances
things work pretty well.

[:buffer :m1-blocked :m1-busy :m2-busy :m2-starved]
{:m1-unblocked
 [{:marking [0 1 0 1 0], :value 0.50118328747075}
  {:marking [1 1 0 1 0], :value 0.5228651697897436}
  {:marking [2 1 0 1 0], :value 0.5181440750582226}],
 :m1-blocked 
 [{:marking [3 1 0 1 0], :value 0.546424705570647} 
  {:marking [3 0 1 1 0], :value 0.5841860833305524}]}

...actually the unblocks look goofed up. 

This is not correct (or at least not best):

[:buffer :m1-blocked :m1-busy :m2-busy :m2-starved]

How did m1-starting a job result in there being another job in the buffer?!?!?!
  {:M [1 1 0 1 0], :fire :m1-start-job, :Mp [2 0 1 1 0], :rate 1.0, :clk 3719.7720757450656, :indx 2984}
  {:act :m1-blocked, :prev-act :m1-start-job, :indx 2985, :Mp [2 0 1 1 0], :clk 3720.7720757450656}

The "out" log: (it shows the buffer being full, m2 pulling one, m1 completing one which would set it back to N=3.

{:act :m2-start-job, :bf :b1, :j 1082, :n 3, :clk 3719.7720757450656, :line 2981, :mjpact :sm, :m :m2}
{:act :m1-complete-job, :bf :b1, :j 1085, :n 2, :clk 3719.7720757450656, :line 2982, :mjpact :bj, :m :m1}
{:act :m1-unblocked, :m :m1, :clk 3719.7720757450656, :line 2983, :mjpact :ub}
{:act :m1-start-job, :j 1086, :jt :jobType1, :ends 3720.7720757450656, :clk 3719.7720757450656, :line 2984, :mjpact :aj, :m :m1}

The above  would be easier to read as this:

{:act :m2-start-job, :bf :b1, :j 1082, :n 3,    :clk 3719.7720757450656, :line 2981, :mjpact :sm, :m :m2}
{:act :m1-unblocked, :m :m1,                    :clk 3719.7720757450656, :line 2983, :mjpact :ub}
{:act :m1-complete-job, :bf :b1, :j 1085, :n 2, :clk 3719.7720757450656, :line 2982, :mjpact :bj, :m :m1} (move job)
{:act :m1-start-job, :j 1086, :jt :jobType1,    :clk 3719.7720757450656, :ends 3720.7720757450656, :line 2984, :mjpact :aj, :m :m1}

FIX MJPdes: 
- MJPdes ought to report things upstream happening first. (sort that way (upstream? x y)
- Everything would be easier to read if :clk was first, then :act. 

** <2017-10-18 Wed>

#_(defn reliable? [m]
    (= ReliableMachine (type m)))

#_(defn machine? [m]
  (let [t (type m)]
    (or (= ExpoMachine t) (= ReliableMachine t) (= Machine t))))

#_(defn buffer? [b]
  (let [t (type b)]
    (or (= Buffer t) (= InfiniteBuffer t) (= DedicatedBuffer t))))

** <2017-10-19 Thu>

I reviewed PNNs and found a nice Python implementation. The notebook is [[file:~/Documents/git/sinet/data/SCADA-logs/m2-j2-n3-block-mild-interpreted.clj::{:act%20:m1-blocked,%20:prev-act%20:m1-start-job,%20:indx%20710,%20:Mp%20%5B3%200%201%201%200%5D,%20:clk%202206.0879216608246}][here]].

** <2017-10-20 Fri>

See discussion on keeping my head straight in Discussion area. 
** <2017-10-21 Sat>

I implement the PNN algorithm from [[http://www.personal.reading.ac.uk/~sis01xh/teaching/CY2D2/Pattern3.pdf][here]].

The key think I'm learning is that you can have the same marking associated with many classes. 
But if a class only has one datapoint, it wins when you hit it. This is good! This 
might be useful for distinguishing the size of buffers with different "best-interpretations."

All of this will be useful when I want to compare to the steady-state behavior using 
a parametric infinitessimal! 

(ppprint (subvec (best-interpretation pnpn (-> (app-info) :problem :scada-log)) 0 300))

Interestingly, starve and unstarve are coming up as both ordinary and exceptional. 
For the time being, this shouldn't matter much given that there is only 1 class in 
these exceptional markings. 

{:m2-unstarved {[0 0 1 1 0] 14},
 :m1-unblocked {[2 1 0 1 0] 30},
 :m2-starved {[0 0 1 0 1] 14},
 :ordinary
 {[0 1 0 1 0] 203,
  [2 0 1 1 0] 511,
  [1 1 0 1 0] 263,
  [3 0 1 0 1] 248,
  [1 0 1 1 0] 466,
  [0 1 0 0 1] 14,
  [2 1 0 1 0] 248,
  [3 0 1 1 0] 248,
  [1 0 1 0 1] 217,
  [0 0 1 1 0] 217,
  [0 0 1 0 1] 14,
  [2 0 1 0 1] 263},
 :m1-blocked {[3 0 1 1 0] 30}}

;;; Results with sigma = 1.0
{[0 1 0 1 0] [:m2-unstarved 0.36787944117144233],
 [2 0 1 1 0] [:m1-blocked 0.6065306597126334],
 [1 1 0 1 0] [:m1-unblocked 0.6065306597126334],
 [3 0 1 0 1] [:m1-blocked 0.3678794411714423],
 [1 0 1 1 0] [:m2-unstarved 0.6065306597126334],
 [0 1 0 0 1] [:m2-starved 0.36787944117144233],
 [2 1 0 1 0] [:m1-unblocked 1.0],
 [3 0 1 1 0] [:m1-blocked 1.0],
 [1 0 1 0 1] [:m2-starved 0.6065306597126334],
 [0 0 1 1 0] [:m2-unstarved 1.0],
 [0 0 1 0 1] [:m2-starved 1.0],
 [2 0 1 0 1] [:ordinary 0.3312510892460261]}

;;; Results with sigma = 0.2 MAKES PERFECT SENSE!
{[0 1 0 1 0] [:ordinary 0.06971187503880233],
 [2 0 1 1 0] [:ordinary 0.17548168297989752],
 [1 1 0 1 0] [:ordinary 0.09031651123868557],
 [3 0 1 0 1] [:ordinary 0.0851651717421801],
 [1 0 1 1 0] [:ordinary 0.16002840419305475],
 [0 1 0 0 1] [:ordinary 0.0048076923087272344],
 [2 1 0 1 0] [:m1-unblocked 1.0],
 [3 0 1 1 0] [:m1-blocked 1.0],
 [1 0 1 0 1] [:ordinary 0.07451958526421719],
 [0 0 1 1 0] [:m2-unstarved 1.0],
 [0 0 1 0 1] [:m2-starved 1.0],
 [2 0 1 0 1] [:ordinary 0.09031652915550199]}

** <2017-10-22 Sun>

Yesterday I got PNNs working nicely! If only every day were that productive!
This morning (well, until 2:30PM!) I cleaned up MJPdes. 
Today we experiment with the idea of replacing euclid-dist2 with some notion of "pn network distance."

- Places are dimensions, movement can be along one or more dimensions. 
- Transitions are the things that determine movement; only dimensions referenced in the
  transition change between states. 
- Each marking has an associated classification. Markings that are very dissimilar from the
  classified marking should have a large distance measure from it. EUCLIDEAN DISTANCE ENSURES THIS.
  The PDF distributes the classification among each training instance. 

- I was concerned about the proximity of activity over time. That changes according to rates. 
  If one part of the network has high-rate transitions, we'd expect more activity from it. But so what?
- I was thinking about "locality of reference" -- that with each transition, only connected places change. 

--> Maybe then what I'm after is to use the distance between transitions in the measure of 
    distance between states. 
    Q: But what does that mean? 
    A: A transition occurs -- we want to learn the relationship between transition and the emission of 
       exceptional messages. This is a temporal relationship (E.g. How many steps after firing X do
       I see exceptional message M?) This isn't judged by pn-path stuff, it is by reachability graph!

Maybe just do the Euclidean distance between markings times the number of steps. 

- There is nothing preventing 
- Transitions that are far from the 

(defn paths-to
  "Return the paths from FROM to TO (both are names of places or transitions) 
   in exactly STEPS steps (counting places, transitions and arcs)."
  [pn from to nsteps & {:keys [back?]}]

** <2017-10-23 Mon>

Another productive day (without really working too hard!). 
The distance function that I'm using is strictly distance between nodes in the rgraph. 
I use loom to calculate this (20 minutes work). 
I'm moving the parzen-pdf-msg stuff from pnn to fitness. 

*** This stuff isn't going to be used
#_(defn min-pn-steps
  "Return the pn distance from FROM to TO in either direction."
  [pn [from to]]
  (if (= from to)
    0
    (loop [cnt 1]
      (if (> cnt 100) 1000, ; POD 1000, it is probably down stream
          (if-let [path (or (not-empty (pnu/paths-to pn from to cnt))
                            (not-empty (pnu/paths-to pn to from cnt)))]
            (/ (-> path first count) 4) ; counts arcs, transitions; I want place to place.
            (recur (inc cnt)))))))

#_(defn pn-distance-table
  "Return a table of all pn distances"
  [pn]
  (let [places (map :name (:places pn))
        keys (for [from places
                   to   places]
               [from to])]
    (zipmap keys
            (map #(min-pn-steps pn %) keys))))


*** This stuff could go in project just of simple neural nets
;;; :marking-key [:buffer :m1-blocked :m1-busy :m2-busy :m2-starved],
;;; It blocks after [2 0 1 1 0]

;;; POD NYI
#_(defn pick-net 
  "Given a list of NN, choose the most accurate one for its message."
  [nets]
  (let [result (filter nn/net? nets)]
    (when (> (count result) 1)
      (println "Multiple nets. Pick NYI."))
    (first nets)))
  
#_(defn train-msg
  "Train the net for the msg-type using the log interpretation."
  [net interp msg-type]
  (let [train-data (:interpreted-log interp)
        last-indx (-> train-data last :indx)
        fires-on (atom {:msg-type msg-type})]
    (loop [net net
           indx 0]
      (if (>= indx last-indx) ; terminate
        (-> net
            (assoc :fires-on @fires-on)
            (assoc :msg-type msg-type)
            (assoc :start-link (:start-link interp)))
        (let [msg (nth train-data indx)
              label (if (= (:act msg) msg-type) 1 0)           ; (rand-int 2)
              inputs (cond (== label 1)             (:Mp msg), ; (noise) 
                           (contains? msg :fire)    (:M  msg), ; (noise) 
                           :otherwise :skip)] ; an exceptional message but not the one I'm learning. 
          (when (== label 1) ; track markings it is firing on
            ;;(println msg)
            (if (contains? @fires-on (:Mp msg))
              (swap! fires-on #(update % (:Mp msg) inc))
              (swap! fires-on #(assoc  % (:Mp msg) 1))))
          (recur
           (if (= inputs :skip)
             net
             (nn/train-step net
                            (vec (map double inputs))
                            (vector (double label))))
           (inc indx)))))))

#_(defn train-all
  "Given a SCADA log interpretation, return a map providing the best NN for each message."
  [interp]
  (let [size   (-> interp :marking-key count)
        msgs   (-> (app-info) :problem :exceptional-msgs)]
    (zipmap msgs
            (map #(train-msg (nn/make-net size 1 size) interp %) msgs))))

#_(defn exceptional-markings
  "Return a vector of {:marking x :value y} indicating that the 
   marking associates with the exceptional class of the neural net."
  [net markings]
  (let [results (zipmap markings
                         (map #(first (nn/eval-net net %)) markings))]
    (reduce (fn [success [mark class-val]]
              (if (> class-val 0.5)
                (conj success {:marking mark :value class-val})
                success))
            []
            results)))

;;; (tryme pnpn (-> (app-info) :problem :scada-log))
#_(defn tryme [pn scada-log]
  (let [interp (best-interpretation pn scada-log) ; POD stop after have all markings. 
        nets (train-all interp)
        markings (distinct (map :M (:rgraph interp)))]
    (reduce (fn [res [msg net]]
              (assoc res msg (exceptional-markings net markings)))
            {}
            nets)))

#_(defn noise []
  (vec (repeatedly 5 #(rand-int 2))))

*** This stuff is what I used prior to recognizing that I need occurrence counts for the parzen-fn
It has been replaced by compute-msg-table.
(defn compute-pnn-data
  "Return a map indicating what markings are associated with what message types, 
   where message types are either ':ordinary' or some exceptional message type."
  [pn scada-log]
  (let [interp (best-interpretation pn scada-log)
        markings (-> (map :M (:rgraph interp)) set)
        excepts (->> (filter #(contains? % :act) (:interpreted-log interp))
                     (map #(dissoc % :clk))
                     (map #(dissoc % :indx))
                     distinct)
        classes (conj (distinct (map :act excepts)) :ordinary)
        emarks (set (map :Mp excepts))
        data (reduce
              (fn [data mark]
                (if (contains? emarks mark)
                  (update-in data
                             [(some #(when (= (:Mp %) mark) (:act %)) excepts)]
                             #(conj % mark))
                  (update-in data [:ordinary] #(conj % mark))))
              (zipmap classes (repeat (count classes) []))
              markings)]
    data))


        
*** This stuff was my first pass at interpretation???
#_(defn best-nav
  "Picking various starting points in the SCADA log, return the 
   longest path of it that can be walked using the QPN." 
  [inv]
  (let [rgraph (pnr/simple-reach (:pn inv))
        exceptional (set/difference scada-msg-types (set (map :fire rgraph)))
        msg1 (first scada-msgs)
        start-marks (map :Mp (filter #(= (:fire %) (:name msg1)) rgraph))]
    (map #(navigate-qpn (:pn inv) rgraph exceptional % 0 (dec (count scada-msgs))) start-marks)))

;;; The set of exceptional message types is decided on a per-QPN basis.
;;; Whatever is in the SCADA log but not a QPN event is exceptional for that QPN. 

;;; POD I think it is enough to always start at position 0 in the SCADA log because
;;;     exceptional situations are the only thing in the way. 
;;;     But is this still sensitive to to the initial marking???
(defn navigate-qpn
  "Using the QPN, try to walk the SCADA log from the argument marking and associated 
   starting position in the log to the argument stop position.
   Return a map describing how far it was possible to navigate and what markings were
   associated with the exceptional messages encountered."
  [pn rgraph excepts mark start stop]
  (let [pn (pnr/renumber-pids pn)]
    (loop [result {:start start :ix (+ start 1) :mark mark :path [] :excepts {}}]
      (let [links (filter #(= (:M %) (:mark result)) rgraph)
            event (:name (nth scada-msgs (:ix result)))
            link  (some #(when (= event (:fire %)) %) links)]
        (if (or (and (not link)
                     (not (some #(= event %) excepts)))
                (>= (inc (:ix result)) stop))
          result 
          (recur (if link
                   (-> result
                       (update :ix inc)
                       (assoc :mark (:Mp link)))
                   (-> result
                       (update :ix inc)
                       (update-in [:excepts event] #(distinct (conj %1 %2)) mark)))))))))


As is apparent from the four subtopics above, I cut out a lot of code today!

Refactors stuff has not yet been tested. 
